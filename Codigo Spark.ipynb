{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://DESKTOP-OMG45KP.mshome.net:4040\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1659964600500)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: String = 3.3.0\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "div.output_area pre {\n",
       "    white-space: pre;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "div.output_area pre {\n",
    "    white-space: pre;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: gray;\">String Methods</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/08 08:16:54 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped\n",
      "+------------------------------------------------------+\n",
      "|description                                           |\n",
      "+------------------------------------------------------+\n",
      "|    This is my long long text, so do not bother me    |\n",
      "+------------------------------------------------------+\n",
      "\n",
      "+--------------------------------------------------+--------------------------------------------------+------------------------------------------------------+----------------------------------------------------------+------------------------------------------------------+------------------------------------------------------+-------------------------------------------------------+--------------------------------------------------------------------------+------------------------------------------------------+----------------------------------------------------------------------------------------------------+------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------+--------+\n",
      "|ltrim                                             |rtrim                                             |initcap                                               |concat                                                    |lower                                                 |upper                                                 |regexString                                            |split                                                                     |translate                                             |lpad                                                                                                |description                                           |rpad                                                                                                |trim                                          |contains|\n",
      "+--------------------------------------------------+--------------------------------------------------+------------------------------------------------------+----------------------------------------------------------+------------------------------------------------------+------------------------------------------------------+-------------------------------------------------------+--------------------------------------------------------------------------+------------------------------------------------------+----------------------------------------------------------------------------------------------------+------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------+--------+\n",
      "|This is my long long text, so do not bother me    |    This is my long long text, so do not bother me|    This Is My Long Long Text, So Do Not Bother Me    |Fool    This is my long long text, so do not bother me    |    this is my long long text, so do not bother me    |    THIS IS MY LONG LONG TEXT, SO DO NOT BOTHER ME    |    COLOR is my long long text, so do not bother me    |[, , , , This, is, my, long, long, text,, so, do, not, bother, me, , , , ]|    1337 37 my long long text, 7o do not bot3er me    |xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx    This is my long long text, so do not bother me    |    This is my long long text, so do not bother me    |    This is my long long text, so do not bother me    xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx|This is my long long text, so do not bother me|false   |\n",
      "+--------------------------------------------------+--------------------------------------------------+------------------------------------------------------+----------------------------------------------------------+------------------------------------------------------+------------------------------------------------------+-------------------------------------------------------+--------------------------------------------------------------------------+------------------------------------------------------+----------------------------------------------------------------------------------------------------+------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n",
       "import org.apache.spark.sql.Column\r\n",
       "import org.apache.spark.sql.DataFrame\r\n",
       "df: org.apache.spark.sql.DataFrame = [description: string]\r\n",
       "dfStringTransformations: scala.collection.immutable.Map[String,org.apache.spark.sql.Column] = Map(ltrim -> ltrim(description) AS ltrim, rtrim -> rtrim(description) AS rtrim, initcap -> initcap(description) AS initcap, concat -> concat(Fool, description) AS concat, lower -> lower(description) AS lower, upper -> upper(description) AS upper, regexString -> regexp_replace(description, This, COLOR, 1) AS regexString, split -> split(description,  , -1) AS split, translate -> translate(description, This, 1337) AS translate, lpad -> lpad(description, 100, x) AS lpad, original -> description, rpad -> rpad(description, 100,...\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.Column\n",
    "import org.apache.spark.sql.DataFrame\n",
    "scala.collection.mutable.Map\n",
    "\n",
    "val df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "'    This is my long long text, so do not bother me    ' AS description\n",
    "\"\"\")\n",
    "df.show(false)\n",
    "\n",
    "var dfStringTransformations = Map((\"original\" -> col(\"description\")))\n",
    "dfStringTransformations += (\"initcap\" -> initcap(col(\"description\")).as(\"initcap\"))\n",
    "dfStringTransformations += (\"concat\" -> concat(lit(\"Fool\"), col(\"description\")).as(\"concat\"))\n",
    "dfStringTransformations += (\"lower\" -> lower(col(\"description\")).as(\"lower\"))\n",
    "dfStringTransformations += (\"upper\" -> upper(col(\"description\")).as(\"upper\"))\n",
    "dfStringTransformations += (\"ltrim\" -> ltrim(col(\"description\")).as(\"ltrim\"))\n",
    "dfStringTransformations += (\"rtrim\" -> rtrim(col(\"description\")).as(\"rtrim\"))\n",
    "dfStringTransformations += (\"trim\" -> trim(col(\"description\")).as(\"trim\"))\n",
    "dfStringTransformations += (\"lpad\" -> lpad(col(\"description\"),  100, \"x\").as(\"lpad\"))\n",
    "dfStringTransformations += (\"rpad\" -> rpad(col(\"description\"),  100, \"x\").as(\"rpad\"))\n",
    "dfStringTransformations += (\"regexString\" -> regexp_replace(col(\"description\"), \"This\", \"COLOR\").alias(\"regexString\"))\n",
    "dfStringTransformations += (\"translate\" -> translate(col(\"description\"), \"This\", \"1337\").as(\"translate\"))\n",
    "dfStringTransformations += (\"contains\"-> col(\"description\").contains(\"BLACK\").as(\"contains\"))\n",
    "dfStringTransformations += (\"split\" -> split(col(\"description\"), \" \").as(\"split\"))\n",
    "\n",
    "val transformations = (for ((key, value) <- dfStringTransformations) yield dfStringTransformations(key)).toSeq\n",
    "df.select(transformations: _*).show(false)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: gray;\">Numerical Methods</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|1.1  |\n",
      "|1.2  |\n",
      "|1.3  |\n",
      "|1.4  |\n",
      "|1.5  |\n",
      "|1.6  |\n",
      "|1.7  |\n",
      "|1.8  |\n",
      "|1.9  |\n",
      "|2.0  |\n",
      "|2.1  |\n",
      "|2.2  |\n",
      "|2.3  |\n",
      "|2.4  |\n",
      "|2.5  |\n",
      "|2.6  |\n",
      "|2.7  |\n",
      "|2.8  |\n",
      "|2.9  |\n",
      "|3.0  |\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+------------------+-----+-----+\n",
      "|value|pow               |brown|round|\n",
      "+-----+------------------+-----+-----+\n",
      "|1.1  |6.21              |1.0  |1.0  |\n",
      "|1.2  |6.4399999999999995|1.0  |1.0  |\n",
      "|1.3  |6.69              |1.0  |1.0  |\n",
      "|1.4  |6.96              |1.0  |1.0  |\n",
      "|1.5  |7.25              |2.0  |2.0  |\n",
      "|1.6  |7.5600000000000005|2.0  |2.0  |\n",
      "|1.7  |7.89              |2.0  |2.0  |\n",
      "|1.8  |8.24              |2.0  |2.0  |\n",
      "|1.9  |8.61              |2.0  |2.0  |\n",
      "|2.0  |9.0               |2.0  |2.0  |\n",
      "|2.1  |9.41              |2.0  |2.0  |\n",
      "|2.2  |9.84              |2.0  |2.0  |\n",
      "|2.3  |10.29             |2.0  |2.0  |\n",
      "|2.4  |10.76             |2.0  |2.0  |\n",
      "|2.5  |11.25             |2.0  |3.0  |\n",
      "|2.6  |11.760000000000002|3.0  |3.0  |\n",
      "|2.7  |12.290000000000001|3.0  |3.0  |\n",
      "|2.8  |12.84             |3.0  |3.0  |\n",
      "|2.9  |13.41             |3.0  |3.0  |\n",
      "|3.0  |14.0              |3.0  |3.0  |\n",
      "+-----+------------------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n",
       "import org.apache.spark.sql.Column\r\n",
       "import org.apache.spark.sql.DataFrame\r\n",
       "df: org.apache.spark.sql.DataFrame = [value: double, pow: double ... 2 more fields]\r\n",
       "dfStringTransformations: scala.collection.immutable.Map[String,org.apache.spark.sql.Column] = Map(original -> value, pow -> (POWER(value, 2.0) + 5) AS pow, brown -> bround(value, 0) AS brown, round -> round(value, 0) AS round)\r\n",
       "transformations: Seq[org.apache.spark.sql.Column] = List(value, (POWER(value, 2.0) + 5) AS pow, bround(value, 0) AS brown, round(value, 0) AS round)\r\n",
       "df: org.apache.spark.sql.DataFrame = [value: double, pow: double ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.Column\n",
    "import org.apache.spark.sql.DataFrame\n",
    "scala.collection.mutable.Map\n",
    "var df = (sc.parallelize(1 to 100)).toDF(\"value\").select((lit(1)+col(\"value\")/10).as(\"value\"))\n",
    "df.show(false)\n",
    "var dfStringTransformations = Map((\"original\" -> col(\"value\")))\n",
    "dfStringTransformations += (\"pow\" -> (pow(col(\"value\"), 2)+5).as(\"pow\"))\n",
    "dfStringTransformations += (\"brown\" -> bround(col(\"value\")).as(\"brown\"))\n",
    "dfStringTransformations += (\"round\" -> round(col(\"value\")).as(\"round\"))\n",
    "\n",
    "val transformations = (for ((key, value) <- dfStringTransformations) yield dfStringTransformations(key)).toSeq\n",
    "df = df.select(transformations: _*)\n",
    "df.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: gray;\">Statistical Methods</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|initial|value             |\n",
      "+-------+------------------+\n",
      "|1      |2.0               |\n",
      "|2      |1.5               |\n",
      "|3      |1.3333333333333333|\n",
      "|4      |1.25              |\n",
      "|5      |1.2               |\n",
      "|6      |1.1666666666666667|\n",
      "|7      |1.1428571428571428|\n",
      "|8      |1.125             |\n",
      "|9      |1.1111111111111112|\n",
      "|10     |1.1               |\n",
      "|11     |1.0909090909090908|\n",
      "|12     |1.0833333333333333|\n",
      "|13     |1.0769230769230769|\n",
      "|14     |1.0714285714285714|\n",
      "|15     |1.0666666666666667|\n",
      "|16     |1.0625            |\n",
      "|17     |1.0588235294117647|\n",
      "|18     |1.0555555555555556|\n",
      "|19     |1.0526315789473684|\n",
      "|20     |1.05              |\n",
      "+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+\n",
      "|corr                |\n",
      "+--------------------+\n",
      "|-0.48008520148401124|\n",
      "+--------------------+\n",
      "\n",
      "+-------------------+\n",
      "|cov                |\n",
      "+-------------------+\n",
      "|-1.6196256464080074|\n",
      "+-------------------+\n",
      "\n",
      "Describe()\n",
      "+-------+------------------+------------------+\n",
      "|summary|           initial|             value|\n",
      "+-------+------------------+------------------+\n",
      "|  count|               100|               100|\n",
      "|   mean|              50.5|1.0518737751763962|\n",
      "| stddev|29.011491975882016|0.1174602896611035|\n",
      "|    min|                 1|              1.01|\n",
      "|    max|               100|               2.0|\n",
      "+-------+------------------+------------------+\n",
      "\n",
      "Describe('initial', 'value')\n",
      "+-------+------------------+------------------+\n",
      "|summary|           initial|             value|\n",
      "+-------+------------------+------------------+\n",
      "|  count|               100|               100|\n",
      "|   mean|              50.5|1.0518737751763962|\n",
      "| stddev|29.011491975882016|0.1174602896611035|\n",
      "|    min|                 1|              1.01|\n",
      "|    max|               100|               2.0|\n",
      "+-------+------------------+------------------+\n",
      "\n",
      "aproxQuantile: 42.0\n",
      "\n",
      "crosstab\n",
      "+------------+------+----+------+\n",
      "|name_product|coffee|milk|suggar|\n",
      "+------------+------+----+------+\n",
      "|        Vivi|     0|   2|     1|\n",
      "|         Ivy|     0|   1|     0|\n",
      "|       Dandy|     1|   0|     0|\n",
      "+------------+------+----+------+\n",
      "\n",
      "+-----+-------+-----------------------------+\n",
      "|name |product|monotonically_increasing_id()|\n",
      "+-----+-------+-----------------------------+\n",
      "|Vivi |milk   |17179869184                  |\n",
      "|Vivi |suggar |34359738368                  |\n",
      "|Vivi |milk   |60129542144                  |\n",
      "|Ivy  |milk   |77309411328                  |\n",
      "|Dandy|coffee |94489280512                  |\n",
      "+-----+-------+-----------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{count, mean, stddev_pop, min, max}\r\n",
       "df: org.apache.spark.sql.DataFrame = [initial: int, value: double]\r\n",
       "dfString: org.apache.spark.sql.DataFrame = [name: string, product: string]\r\n"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{count, mean, stddev_pop, min, max} //same as the values that outputs describe\n",
    "\n",
    "var df = (sc.parallelize(1 to 100)).toDF(\"value\").select(col(\"value\").as(\"initial\"), (lit(1)+lit(1)/col(\"value\")).as(\"value\"))\n",
    "df.show(false)\n",
    "df.select(corr(\"value\", \"initial\").as(\"corr\")).show(false) //Pearson Correlation. Agg function over data.\n",
    "df.select(covar_pop(\"value\", \"initial\").as(\"cov\")).show(false) //Covariance. + both variables increases, - one of the variable increase while the other decrease\n",
    "println(\"Describe()\")\n",
    "df.describe().show() // describe all columns\n",
    "println(\"Describe('initial', 'value')\")\n",
    "df.describe(\"initial\", \"value\").show() // describe one column\n",
    "println(\"aproxQuantile: \" + df.stat.approxQuantile(\"initial\", Array(0.5), 0.25)(0)) // col: base column of calculation, probabilities: 0 is the min, 0.5 is the mean, 1 is the max. relativeError: 0, exact quantiles are calculated (very expensive). output is a list\n",
    "println()\n",
    "\n",
    "val dfString = sc.parallelize(\n",
    "    Seq(\n",
    "        (\"Vivi\", \"milk\"),\n",
    "        (\"Vivi\", \"suggar\"),\n",
    "        (\"Vivi\", \"milk\"),\n",
    "        (\"Ivy\", \"milk\"),\n",
    "        (\"Dandy\", \"coffee\")\n",
    "    )\n",
    ").toDF(\"name\", \"product\")\n",
    "println(\"crosstab\")\n",
    "dfString.stat.crosstab(\"name\", \"product\").show() // Table of the frequency distribution for a set of variables.\n",
    "dfString.select(col(\"*\"), monotonically_increasing_id()).show(false) //unique ID for each row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: gray;\">Dates and Timestamps</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different DataFormats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- current_date: date (nullable = false)\n",
      " |-- date_format(current_timestamp(), yyyy MM dd): string (nullable = false)\n",
      " |-- date_format(current_timestamp(), MM/dd/yyyy hh:mm): string (nullable = false)\n",
      " |-- date_format(current_timestamp(), yyyy MMM dd): string (nullable = false)\n",
      " |-- date_format(current_timestamp(), yyyy MMMM dd E): string (nullable = false)\n",
      "\n",
      "+------------+--------------------------------------------+--------------------------------------------------+---------------------------------------------+------------------------------------------------+\n",
      "|current_date|date_format(current_timestamp(), yyyy MM dd)|date_format(current_timestamp(), MM/dd/yyyy hh:mm)|date_format(current_timestamp(), yyyy MMM dd)|date_format(current_timestamp(), yyyy MMMM dd E)|\n",
      "+------------+--------------------------------------------+--------------------------------------------------+---------------------------------------------+------------------------------------------------+\n",
      "|2022-07-28  |2022 07 28                                  |07/28/2022 02:14                                  |2022 Jul 28                                  |2022 July 28 Thu                                |\n",
      "+------------+--------------------------------------------+--------------------------------------------------+---------------------------------------------+------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [current_date: date, date_format(current_timestamp(), yyyy MM dd): string ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq(1).toDF(\"seq\").select(\n",
    "    current_date().as(\"current_date\"),\n",
    "    date_format(current_timestamp(),\"yyyy MM dd\"),\n",
    "    date_format(current_timestamp(),\"MM/dd/yyyy hh:mm\"),\n",
    "    date_format(current_timestamp(),\"yyyy MMM dd\"),\n",
    "    date_format(current_timestamp(),\"yyyy MMMM dd E\")\n",
    "  )\n",
    "df.printSchema\n",
    "df.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with unix timestamp\n",
    "Unix time is also known as Epoch time which specifies the moment in time since 1970-01-01 00:00:00 UTC. It is the number of seconds passed since Epoch time. Epoch time is widely used in Unix like operating systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Changing from timestamp to unix timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- current_timestamp: timestamp (nullable = false)\n",
      " |-- unix_timestamp_column: long (nullable = true)\n",
      " |-- MM-dd-yyyy HH:mm:ss: long (nullable = true)\n",
      " |-- MM-dd-yyyy: long (nullable = true)\n",
      " |-- unix_timestamp: long (nullable = true)\n",
      "\n",
      "+--------------------------+---------------------+-------------------+----------+--------------+\n",
      "|current_timestamp         |unix_timestamp_column|MM-dd-yyyy HH:mm:ss|MM-dd-yyyy|unix_timestamp|\n",
      "+--------------------------+---------------------+-------------------+----------+--------------+\n",
      "|2022-07-28 14:24:27.811971|1659036267           |1659036267         |1659036267|1659036267    |\n",
      "+--------------------------+---------------------+-------------------+----------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [current_timestamp: timestamp, unix_timestamp_column: bigint ... 3 more fields]\r\n",
       "df: org.apache.spark.sql.DataFrame = [current_timestamp: timestamp, unix_timestamp_column: bigint ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df = Seq(1).toDF(\"seq\").select(current_timestamp().as(\"current_timestamp\"))\n",
    "df = df.select(\n",
    "    col(\"current_timestamp\"),\n",
    "    unix_timestamp(col(\"current_timestamp\")).as(\"unix_timestamp_column\"),\n",
    "    unix_timestamp(col(\"current_timestamp\"),\"MM-dd-yyyy HH:mm:ss\").as(\"MM-dd-yyyy HH:mm:ss\"),\n",
    "    unix_timestamp(col(\"current_timestamp\"),\"MM-dd-yyyy\").as(\"MM-dd-yyyy\"),\n",
    "    unix_timestamp().as(\"unix_timestamp\")\n",
    ")\n",
    "df.printSchema\n",
    "df.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Changing from unix timestamp to timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- from_unixtime(unix_timestamp_column, MM-dd-yyyy HH:mm:ss): string (nullable = true)\n",
      " |-- from_unixtime(unix_timestamp_column, yyyy/MM/dd HH:mm:ss): string (nullable = true)\n",
      " |-- from_unixtime(MM-dd-yyyy HH:mm:ss, MM-dd-yyyy): string (nullable = true)\n",
      " |-- from_unixtime(MM-dd-yyyy, yyyy-MM-dd HH:mm:ss): string (nullable = true)\n",
      "\n",
      "+---------------------------------------------------------+---------------------------------------------------------+----------------------------------------------+----------------------------------------------+\n",
      "|from_unixtime(unix_timestamp_column, MM-dd-yyyy HH:mm:ss)|from_unixtime(unix_timestamp_column, yyyy/MM/dd HH:mm:ss)|from_unixtime(MM-dd-yyyy HH:mm:ss, MM-dd-yyyy)|from_unixtime(MM-dd-yyyy, yyyy-MM-dd HH:mm:ss)|\n",
      "+---------------------------------------------------------+---------------------------------------------------------+----------------------------------------------+----------------------------------------------+\n",
      "|07-28-2022 14:28:13                                      |2022/07/28 14:28:13                                      |07-28-2022                                    |2022-07-28 14:28:13                           |\n",
      "+---------------------------------------------------------+---------------------------------------------------------+----------------------------------------------+----------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [from_unixtime(unix_timestamp_column, MM-dd-yyyy HH:mm:ss): string, from_unixtime(unix_timestamp_column, yyyy/MM/dd HH:mm:ss): string ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2 = df.select(\n",
    "    from_unixtime(col(\"unix_timestamp_column\"),\"MM-dd-yyyy HH:mm:ss\"),\n",
    "    from_unixtime(col(\"unix_timestamp_column\"),\"yyyy/MM/dd HH:mm:ss\"),\n",
    "    from_unixtime(col(\"MM-dd-yyyy HH:mm:ss\"),\"MM-dd-yyyy\"),\n",
    "    from_unixtime(col(\"MM-dd-yyyy\"))\n",
    ")\n",
    "df2.printSchema()\n",
    "df2.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------+----------+--------+--------------+----------+----------+-------------------+----+-------+-----+---------+------------------------+-----------------------+------------------------+----------------------+\n",
      "|current_date|add_months|  date_add|  date_sub|datediff|months_between|  next_day|     trunc|         date_trunc|year|quarter|month|dayofweek|dayofmonth(current_date)|dayofyear(current_date)|weekofyear(current_date)|last_day(current_date)|\n",
      "+------------+----------+----------+----------+--------+--------------+----------+----------+-------------------+----+-------+-----+---------+------------------------+-----------------------+------------------------+----------------------+\n",
      "|  2022-07-28|2024-07-28|2022-07-29|2022-07-27|       3|          24.0|2022-08-01|2022-01-01|2022-01-01 00:00:00|2022|      3|    7|        5|                      28|                    209|                      30|            2022-07-31|\n",
      "+------------+----------+----------+----------+--------+--------------+----------+----------+-------------------+----+-------+-----+---------+------------------------+-----------------------+------------------------+----------------------+\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [current_date: date, add_months: date ... 15 more fields]\r\n",
       "df: org.apache.spark.sql.DataFrame = [current_date: date, add_months: date ... 15 more fields]\r\n"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df = Seq(1).toDF().select(current_date().as(\"current_date\"))\n",
    "df = df.select(\n",
    "    col(\"*\"),\n",
    "    add_months(col(\"current_date\"), 24).as(\"add_months\"), // ADD MONTHS\n",
    "    date_add(col(\"current_date\"), 1).as(\"date_add\"), //ADD DAY\n",
    "    date_sub(col(\"current_date\"), 1).as(\"date_sub\"), // SUB DAY\n",
    "    datediff(col(\"current_date\"),  date_sub(col(\"current_date\"), 3)).as(\"datediff\"), // RETURNS # DAYS\n",
    "    months_between(add_months(col(\"current_date\"), 24), col(\"current_date\")).as(\"months_between\"), // RETURN # MONTHS\n",
    "    next_day(col(\"current_date\"), \"Monday\").as(\"next_day\"), //Which future day will be Monday\n",
    "    trunc(col(\"current_date\"), \"yy\").as(\"trunc\"), //Returns date truncated to the unit specified by the format. For example, `trunc(\"2018-11-19 12:01:19\", \"year\")` returns 2018-01-01 format: 'year', 'yyyy', 'yy' to truncate by year, 'month', 'mon', 'mm' to truncate by month\n",
    "    date_trunc(\"yy\", col(\"current_date\")).as(\"date_trunc\"), //SAME AS TRUNC\n",
    "    year(col(\"current_date\")).as(\"year\"),\n",
    "    quarter(col(\"current_date\")).as(\"quarter\"),\n",
    "    month(col(\"current_date\")).as(\"month\"),\n",
    "    dayofweek(col(\"current_date\")).as(\"dayofweek\"), // Ranges from 1 for a Sunday through to 7 for a Saturday\n",
    "    dayofmonth(col(\"current_date\")),\n",
    "    dayofyear(col(\"current_date\")),\n",
    "    weekofyear(col(\"current_date\")),\n",
    "    last_day(col(\"current_date\")), // last date of the current month of the current year\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Timestamp functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+----+------+------+-------------------+\n",
      "|current_date              |hour|minute|second|to_timestamp       |\n",
      "+--------------------------+----+------+------+-------------------+\n",
      "|2022-07-28 17:12:02.927974|17  |12    |2     |2019-07-01 12:01:19|\n",
      "+--------------------------+----+------+------+-------------------+\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [current_date: timestamp, hour: int ... 3 more fields]\r\n",
       "df: org.apache.spark.sql.DataFrame = [current_date: timestamp, hour: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df = Seq(1).toDF().select(current_timestamp().as(\"current_date\"))\n",
    "df = df.select(\n",
    "    col(\"current_date\"),\n",
    "    hour(col(\"current_date\")).as(\"hour\"),\n",
    "    minute(col(\"current_date\")).as(\"minute\"),\n",
    "    second(col(\"current_date\")).as(\"second\"),\n",
    "    to_timestamp(lit(\"2019-07-01 12:01:19.000\"), \"yyyy-MM-dd HH:mm:ss.SSS\").as(\"to_timestamp\")\n",
    ")\n",
    "df.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: gray;\">Working with Nulls</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|column1|column2|\n",
      "+-------+-------+\n",
      "|null   |column2|\n",
      "|column1|column2|\n",
      "|null   |null   |\n",
      "+-------+-------+\n",
      "\n",
      "+-------+-------+--------+\n",
      "|column1|column2|coalesce|\n",
      "+-------+-------+--------+\n",
      "|   null|column2| column2|\n",
      "|column1|column2| column1|\n",
      "|   null|   null|    null|\n",
      "+-------+-------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n",
       "dummyDF: org.apache.spark.sql.DataFrame = [_1: string, _2: string]\r\n",
       "df: org.apache.spark.sql.DataFrame = [column1: string, column2: string]\r\n",
       "df: org.apache.spark.sql.DataFrame = [column1: string, column2: string]\r\n",
       "dfStringTransformations: scala.collection.immutable.Map[String,org.apache.spark.sql.Column] = Map(column1 -> column1, column2 -> column2 AS column2, coalesce -> coalesce(column1, column2) AS coalesce)\r\n",
       "transformations: Seq[org.apache.spark.sql.Column] = List(column1, column2 AS column2, coalesce(column1, column2) AS coalesce)\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "val dummyDF = List((\"column1\", \"column2\"), (null, null)).toDF\n",
    "var df = spark.sql(\"Select cast(null as string) as column1, 'column2' as column2\")\n",
    "df = df.union(dummyDF)\n",
    "df.show(false)\n",
    "\n",
    "var dfStringTransformations = Map((\"column1\" -> col(\"column1\")))\n",
    "dfStringTransformations += (\"column2\" -> col(\"column2\").as(\"column2\"))\n",
    "dfStringTransformations += (\"coalesce\" -> coalesce(col(\"column1\"), col(\"column2\")).as(\"coalesce\"))\n",
    "                            \n",
    "val transformations = (for ((key, value) <- dfStringTransformations) yield dfStringTransformations(key)).toSeq\n",
    "\n",
    "df.select(transformations: _*).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "-- in SQL\n",
    "SELECT\n",
    "    ifnull(null, 'return_value'),\n",
    "    nullif('value', 'value')\n",
    "    nvl(null, 'return_value'),\n",
    "    nvl2('not_null', 'return_value', \"else_value\")\n",
    "FROM dfTable LIMIT 1\n",
    "\n",
    "\n",
    ">> \n",
    "    +------------+----+------------+------------+\n",
    "    | a          | b  | c          | d          |\n",
    "    +------------+----+------------+------------+\n",
    "    |return_value|null|return_value|return_value|\n",
    "    +------------+----+------------+------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Any row in which Any value is null: ```df.na.drop()``` = ```df.na.drop(\"any\")```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Any row if <b style=\"color:red;\">ALL values</b> are null. <br>\n",
    "Also, <b style=\"color:red;\">ALL could be applied to certain columns</b>, by passing an array of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|column1|column2|\n",
      "+-------+-------+\n",
      "|column1|column2|\n",
      "+-------+-------+\n",
      "\n",
      "+-------+-------+\n",
      "|column1|column2|\n",
      "+-------+-------+\n",
      "|column1|column2|\n",
      "+-------+-------+\n",
      "\n",
      "+-------+-------+\n",
      "|column1|column2|\n",
      "+-------+-------+\n",
      "|   null|column2|\n",
      "|column1|column2|\n",
      "+-------+-------+\n",
      "\n",
      "+-------+-------+\n",
      "|column1|column2|\n",
      "+-------+-------+\n",
      "|column1|column2|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop().show()\n",
    "df.na.drop(\"any\").show()\n",
    "df.na.drop(\"all\").show()\n",
    "df.na.drop(\"all\", Seq(\"column1\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|column1|column2|\n",
      "+-------+-------+\n",
      "|   null|column2|\n",
      "|column1|column2|\n",
      "|   null|   null|\n",
      "+-------+-------+\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|             column1|             column2|\n",
      "+--------------------+--------------------+\n",
      "|All Null values b...|             column2|\n",
      "|             column1|             column2|\n",
      "|All Null values b...|All Null values b...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "+-------+--------+\n",
      "|column1| column2|\n",
      "+-------+--------+\n",
      "|      5| column2|\n",
      "|column1| column2|\n",
      "|      5|No Value|\n",
      "+-------+--------+\n",
      "\n",
      "+-------+-------+\n",
      "|column1|column2|\n",
      "+-------+-------+\n",
      "|   null|column2|\n",
      "|UNKNOWN|column2|\n",
      "|   null|   null|\n",
      "+-------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fillColValues: scala.collection.immutable.Map[String,Any] = Map(column1 -> 5, column2 -> No Value)\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.show\n",
    "df.na.fill(\"All Null values become this string\").show()\n",
    "// in Scala\n",
    "val fillColValues = Map(\"column1\" -> 5, \"column2\" -> \"No Value\")\n",
    "df.na.fill(fillColValues).show\n",
    "df.na.replace(\"column1\", Map(\"column1\" -> \"UNKNOWN\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: gray;\">Ordering</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|column1|column2|\n",
      "+-------+-------+\n",
      "|   null|column2|\n",
      "|   null|   null|\n",
      "|column1|column2|\n",
      "+-------+-------+\n",
      "\n",
      "+-------+-------+\n",
      "|column1|column2|\n",
      "+-------+-------+\n",
      "|   null|column2|\n",
      "|column1|column2|\n",
      "|   null|   null|\n",
      "+-------+-------+\n",
      "\n",
      "+-------+-------+\n",
      "|column1|column2|\n",
      "+-------+-------+\n",
      "|   null|   null|\n",
      "|   null|column2|\n",
      "|column1|column2|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(col(\"column1\")).show() //By default, asc\n",
    "df.sort(col(\"column2\").desc).show()\n",
    "df.sort(col(\"column2\").asc_nulls_first).show()// asc_nulls_first, desc_nulls_first,asc_nulls_last, or desc_nulls_last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: gray;\">Structs</h1>\n",
    "You can think of structs as DataFrames within DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+---------------------+-----+------+------+\n",
      "|name                 |id   |gender|salary|\n",
      "+---------------------+-----+------+------+\n",
      "|{James , , Smith}    |36636|M     |3100  |\n",
      "|{Michael , Rose, }   |40288|M     |4300  |\n",
      "|{Robert , , Williams}|42114|M     |1400  |\n",
      "|{Maria , Anne, Jones}|39192|F     |5500  |\n",
      "|{Jen, Mary, Brown}   |     |F     |-1    |\n",
      "+---------------------+-----+------+------+\n",
      "\n",
      "+--------------+\n",
      "|name.firstname|\n",
      "+--------------+\n",
      "|        James |\n",
      "|      Michael |\n",
      "|       Robert |\n",
      "|        Maria |\n",
      "|           Jen|\n",
      "+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\r\n",
       "import org.apache.spark.sql.Row\r\n",
       "structureSchema: org.apache.spark.sql.types.StructType = StructType(StructField(name,StructType(StructField(firstname,StringType,true),StructField(middlename,StringType,true),StructField(lastname,StringType,true)),true),StructField(id,StringType,true),StructField(gender,StringType,true),StructField(salary,IntegerType,true))\r\n",
       "structureData: Seq[org.apache.spark.sql.Row] = List([[James ,,Smith],36636,M,3100], [[Michael ,Rose,],40288,M,4300], [[Robert ,,Williams],42114,M,1400], [[Maria ,Anne,Jones],39192,F,5500], [[Jen,Mary,Brown],,F,-1])\r\n",
       "df: org.apache.spark.sql.DataFrame = [name: struct<firstname: string, middlename: string ... 1 more field>, id: string ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val structureSchema = new StructType()\n",
    "    .add(\"name\",new StructType()\n",
    "    .add(\"firstname\",StringType)\n",
    "    .add(\"middlename\",StringType)\n",
    "    .add(\"lastname\",StringType))\n",
    "    .add(\"id\",StringType)\n",
    "    .add(\"gender\",StringType)\n",
    "    .add(\"salary\",IntegerType)\n",
    "\n",
    "\n",
    "val structureData = Seq(\n",
    "    Row(Row(\"James \",\"\",\"Smith\"),\"36636\",\"M\",3100),\n",
    "    Row(Row(\"Michael \",\"Rose\",\"\"),\"40288\",\"M\",4300),\n",
    "    Row(Row(\"Robert \",\"\",\"Williams\"),\"42114\",\"M\",1400),\n",
    "    Row(Row(\"Maria \",\"Anne\",\"Jones\"),\"39192\",\"F\",5500),\n",
    "    Row(Row(\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",-1)\n",
    "  )\n",
    "val df = spark.createDataFrame(spark.sparkContext.parallelize(structureData),structureSchema)\n",
    "df.printSchema()\n",
    "df.show(false)\n",
    "\n",
    "df.select(col(\"name\").getField(\"firstname\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: gray;\">Arrays</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- languagesAtWork: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- currentState: string (nullable = true)\n",
      " |-- previousState: string (nullable = true)\n",
      "\n",
      "+----------------+------------------+---------------+------------+-------------+\n",
      "|            name| languagesAtSchool|languagesAtWork|currentState|previousState|\n",
      "+----------------+------------------+---------------+------------+-------------+\n",
      "|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]|          OH|           CA|\n",
      "|   Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]|          NY|           NJ|\n",
      "|Robert,,Williams|      [CSharp, VB]|[Spark, Python]|          UT|           NV|\n",
      "+----------------+------------------+---------------+------------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data: Seq[org.apache.spark.sql.Row] = List([James,,Smith,List(Java, Scala, C++),List(Spark, Java),OH,CA], [Michael,Rose,,List(Spark, Java, C++),List(Spark, Java),NY,NJ], [Robert,,Williams,List(CSharp, VB),List(Spark, Python),UT,NV])\r\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,true),StructField(languagesAtSchool,ArrayType(StringType,true),true),StructField(languagesAtWork,ArrayType(StringType,true),true),StructField(currentState,StringType,true),StructField(previousState,StringType,true))\r\n",
       "df: org.apache.spark.sql.DataFrame = [name: string, languagesAtSchool: array<string> ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Seq(Row(\"James,,Smith\",List(\"Java\",\"Scala\",\"C++\"),List(\"Spark\",\"Java\"),\"OH\",\"CA\"),\n",
    "                             Row(\"Michael,Rose,\",List(\"Spark\",\"Java\",\"C++\"),List(\"Spark\",\"Java\"),\"NY\",\"NJ\"),\n",
    "                             Row(\"Robert,,Williams\",List(\"CSharp\",\"VB\"),List(\"Spark\",\"Python\"),\"UT\",\"NV\"))\n",
    "val schema = new StructType()\n",
    "    .add(\"name\",StringType)\n",
    "    .add(\"languagesAtSchool\", ArrayType(StringType))\n",
    "    .add(\"languagesAtWork\", ArrayType(StringType))\n",
    "    .add(\"currentState\", StringType)\n",
    "    .add(\"previousState\", StringType)\n",
    "val df = spark.createDataFrame(spark.sparkContext.parallelize(data),schema)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+------------------+---------------+------------+-------------+\n",
      "|               split|            name| languagesAtSchool|languagesAtWork|currentState|previousState|\n",
      "+--------------------+----------------+------------------+---------------+------------+-------------+\n",
      "|    [James, , Smith]|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]|          OH|           CA|\n",
      "|   [Michael, Rose, ]|   Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]|          NY|           NJ|\n",
      "|[Robert, , Williams]|Robert,,Williams|      [CSharp, VB]|[Spark, Python]|          UT|           NV|\n",
      "+--------------------+----------------+------------------+---------------+------------+-------------+\n",
      "\n",
      "+--------+\n",
      "|split[0]|\n",
      "+--------+\n",
      "|   James|\n",
      "| Michael|\n",
      "+--------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------+\n",
      "|size(split)|\n",
      "+-----------+\n",
      "|          3|\n",
      "|          3|\n",
      "+-----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------------------------+\n",
      "|array_contains(split, James)|\n",
      "+----------------------------+\n",
      "|                        true|\n",
      "|                       false|\n",
      "|                       false|\n",
      "+----------------------------+\n",
      "\n",
      "+--------+\n",
      "| explode|\n",
      "+--------+\n",
      "|   James|\n",
      "|        |\n",
      "|   Smith|\n",
      "| Michael|\n",
      "|    Rose|\n",
      "|        |\n",
      "|  Robert|\n",
      "|        |\n",
      "|Williams|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(split(col(\"name\"), \",\").as(\"split\"), col(\"*\")).show()\n",
    "df.select(split(col(\"name\"), \",\").as(\"split\"))\n",
    ".selectExpr(\"split[0]\").show(2) // Getting an specific element\n",
    "df.select(split(col(\"name\"), \",\").as(\"split\"))\n",
    ".select(size(col(\"split\"))).show(2)//size of array\n",
    "df.select(split(col(\"name\"), \",\").as(\"split\"))\n",
    ".select(array_contains(col(\"split\"), \"James\")).show()//arrays contains an element\n",
    "df.select(split(col(\"name\"), \",\").as(\"split\"))\n",
    ".select(explode(col(\"split\")).as(\"explode\")).show()//arrays contains an element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: gray;\">Maps</h1>\n",
    "Mptype can be understood as the dynamic version of structType, because with structs you need to define in the schema each of the keys that map to the other structure, whilist in map, you dont."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- addresses: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- city: string (nullable = true)\n",
      " |    |    |-- state: string (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- secondProp: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+-------+-------------------------------+-----------------------------+---------------+\n",
      "|name   |addresses                      |properties                   |secondProp     |\n",
      "+-------+-------------------------------+-----------------------------+---------------+\n",
      "|James  |[{Newark, NY}, {Brooklyn, NY}] |{hair -> black, eye -> brown}|{height -> 5.9}|\n",
      "|Michael|[{SanJose, CA}, {Sandiago, CA}]|{hair -> brown, eye -> black}|{height -> 6}  |\n",
      "+-------+-------------------------------+-----------------------------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.DataTypes\r\n",
       "mapColSimple: org.apache.spark.sql.types.MapType = MapType(StringType,StringType,true)\r\n",
       "mapColComplex: org.apache.spark.sql.types.MapType = MapType(StringType,StructType(StructField(col1,StringType,true),StructField(col2,StringType,true)),true)\r\n",
       "getSchema: org.apache.spark.sql.types.StructType\r\n",
       "getData: Seq[org.apache.spark.sql.Row]\r\n",
       "mapTypeDF: org.apache.spark.sql.DataFrame = [name: string, addresses: array<struct<city:string,state:string>> ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.DataTypes\n",
    "val mapColSimple = DataTypes.createMapType(StringType, StringType) // Eg: Map(5-> \"cinco\")\n",
    "val mapColComplex = DataTypes.createMapType(\n",
    "    StringType,\n",
    "    StructType(\n",
    "        Array(\n",
    "            StructField(\"col1\",StringType),\n",
    "            StructField(\"col2\",StringType )\n",
    "        )\n",
    "    ) \n",
    ") //Map(\"vocales\" -> ['a', 'e', 'i', 'o', 'u'])\n",
    "\n",
    "\n",
    "def getSchema: StructType = {\n",
    "    val mapType  = DataTypes.createMapType(StringType,StringType)\n",
    "    val arrayStructureSchema = new StructType()\n",
    "        .add(\"name\",StringType)\n",
    "        .add(\"addresses\", ArrayType(new StructType()\n",
    "          .add(\"city\",StringType)\n",
    "          .add(\"state\",StringType)))\n",
    "        .add(\"properties\", mapType)\n",
    "        .add(\"secondProp\", MapType(StringType,StringType))\n",
    "    arrayStructureSchema\n",
    "}\n",
    "\n",
    "def getData: Seq[Row] = {\n",
    "    val arrayStructureData = Seq(\n",
    "        Row(\n",
    "            \"James\",\n",
    "            List(Row(\"Newark\",\"NY\"), Row(\"Brooklyn\",\"NY\")),\n",
    "            Map(\"hair\"->\"black\",\"eye\"->\"brown\"), \n",
    "            Map(\"height\"->\"5.9\")\n",
    "        ),\n",
    "        Row(\n",
    "            \"Michael\",\n",
    "            List(Row(\"SanJose\",\"CA\"),Row(\"Sandiago\",\"CA\")),\n",
    "            Map(\"hair\"->\"brown\",\"eye\"->\"black\"),\n",
    "            Map(\"height\"->\"6\")\n",
    "        )\n",
    "    )\n",
    "    arrayStructureData\n",
    "}\n",
    "\n",
    "\n",
    "val mapTypeDF = spark.createDataFrame(spark.sparkContext.parallelize(getData), getSchema)\n",
    "mapTypeDF.printSchema()\n",
    "mapTypeDF.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|name   |map_keys(properties)|\n",
      "+-------+--------------------+\n",
      "|James  |[hair, eye]         |\n",
      "|Michael|[hair, eye]         |\n",
      "+-------+--------------------+\n",
      "\n",
      "+-------+----------------------+\n",
      "|name   |map_values(properties)|\n",
      "+-------+----------------------+\n",
      "|James  |[black, brown]        |\n",
      "|Michael|[brown, black]        |\n",
      "+-------+----------------------+\n",
      "\n",
      "+-------+--------------------------------------------+\n",
      "|name   |map_concat(properties, secondProp)          |\n",
      "+-------+--------------------------------------------+\n",
      "|James  |{hair -> black, eye -> brown, height -> 5.9}|\n",
      "|Michael|{hair -> brown, eye -> black, height -> 6}  |\n",
      "+-------+--------------------------------------------+\n",
      "\n",
      "+----------------------------+\n",
      "|element_at(properties, hair)|\n",
      "+----------------------------+\n",
      "|black                       |\n",
      "|brown                       |\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mapTypeDF.select(col(\"name\"),map_keys(col(\"properties\"))).show(false)\n",
    "mapTypeDF.select(col(\"name\"),map_values(col(\"properties\"))).show(false)\n",
    "mapTypeDF.select(col(\"name\"),map_concat(col(\"properties\"),col(\"secondProp\"))).show(false) //apppend\n",
    "mapTypeDF.select(element_at(col(\"properties\"), \"hair\")).show(false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
